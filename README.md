# data-tooling

Intended for in-place instrumentation - i.e., least disruption - of Jupyter notebooks and other Python scripts
for data lineage (OpenLineage) and quality (Great Expectations).

This repo's Python requirements currently represent a sort of kit-n-caboodle of data libs until
such time we break them out separately. Remove superfluous libs (and related functions in source
files) as needed.

## Key Points

- Lineage functions are setup as wrappers and accept the same + additional args as lib being wrapped (pandas, etc.).
- OpenLineage has the notion of _job_, which can correspond 1:1 with a Jupyter notebook. Multiple runs will occur underneath each job (e.g., each time you run the notebook).
- OpenLineage supports storage of standard and ad hoc sets of metadata via _facets_. These can be found in the Marquez UI upon drilling into a dataset (or job).
- OpenLineage uses the notion of _namespaces_. By default, for a given project there will be two namespaces: 'file' for all datasets, and 'notebook-local' for jobs.
- Great Expectations configuration is generated by application modules as they run and written to ./gx. These aspects are currently not committed to the repo in order to maintain separation across multiple independent local environments/users.

## Dependencies

- With virtual env active, one-time execution of: `great_expectations init`
- Dockerized services represented in `dev-utils/docker-compose.yml`: `docker-compose up`. This will...
  - Start and seed a postgres database (stores OpenLineage events)
  - Start the OpenLineage (marquez) API service (port 5000)
  - Start the Marquez UI (http://localhost:3000)

## Instrumentation

### Notebook: Input and Output Lineage

Examples can be found in the notebooks directory, generally looks like:

```python
# At or near top of notebook
import pandas as pd
from src.data_lineage import initialize_run, start_run, complete_run, read_csv, to_csv

job_name = 'your_notebook'  # Can be whatever you want, but notebook name is good
run_id = initialize_run()  # Initialize run and receive run ID
# Load CSVs: address *all* CSV inputs, here
input_one: pd.DataFrame = read_csv('relative/input/path/one', run_id)
input_two: pd.DataFrame = read_csv('relative/input/path/two', run_id)
start_run(job_name, run_id)

# ...

# Along the way
to_csv('relative/output/path/one', run_id)
to_csv('relative/output/path/two', run_id)

# ...

# At or near bottom of notebook
complete_run(job_name, run_id)
```

### File: Input Validation

Next to each CSV being consumed as an input, add a file with the same name except with a '.yml'
extension instead of '.csv'. Include it in a Great Expectations specification to be used for validation,
results from which will be appended to OpenLineage data and viewable in the Marquez UI.

Currently, if any dataset fails validation:
- The run is marked as failed (and the job will show up in the Marquez UI as failed).
- The dataset is tagged with 'FAILED' (viewable in Marquez UI).
- The outcome of specific expectations can be viewed in the Marquez UI by drilling into the dataset's assertions facet.

Example specifications can be found in the raw data directory.

### Customizing

OpenLineage events can be generated fairly easy at any time during a run, in order to contribute additional metadata related to the job or datasets. Examples to come...

# TODO

- explore metadata persistence
- explore gx persistence/committing
- deployment paradigms
